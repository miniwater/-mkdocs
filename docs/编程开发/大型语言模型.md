# 大型语言模型

## ollama

<https://github.com/jmorganca/ollama>

```shell
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

windows:

- i5 13600k慢吞吞版:`docker run -d -v E:/project/docker/ollama:/root/.ollama -p 11434:11434 --name ollama_CPU ollama/ollama`

- rtx3060ti急速版(爆显存):`docker run -d --gpus=all -v E:/project/docker/ollama:/root/.ollama -p 11434:11434 --name ollama_GPU ollama/ollama`

运行聊天

```shell
ollama run llama2
```

退出聊天

```ollama
/bye
```

模型仓库：<https://ollama.ai/library>

主流模型

| Model              | Parameters | Size  | Download                       |
|--------------------|------------|-------|--------------------------------|
| Llama 2            | 7B         | 3.8GB | `ollama run llama2`            |
| Mistral            | 7B         | 4.1GB | `ollama run mistral`           |
| Phi-2              | 2.7B       | 1.7GB | `ollama run phi`               |
| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`       |
| Starling           | 7B         | 4.1GB | `ollama run starling-lm`       |
| Code Llama         | 7B         | 3.8GB | `ollama run codellama`         |
| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored` |
| Llama 2 13B        | 13B        | 7.3GB | `ollama run llama2:13b`        |
| Llama 2 70B        | 70B        | 39GB  | `ollama run llama2:70b`        |
| Orca Mini          | 3B         | 1.9GB | `ollama run orca-mini`         |
| Vicuna             | 7B         | 3.8GB | `ollama run vicuna`            |
| LLaVA              | 7B         | 4.5GB | `ollama run llava`             |

